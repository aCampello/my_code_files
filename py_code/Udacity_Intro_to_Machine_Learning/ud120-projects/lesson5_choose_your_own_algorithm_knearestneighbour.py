# -*- coding: utf-8 -*-
"""
Created on Wed Jul 26 12:26:36 2017

Udacity Intro to Machine Learning

Lesson 5: K Nearest Neighour  (mini project to choose your own algorithm & teach yourself)

KNN user guide: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier

...

The goal of the mini project is to do terrain classification with an algorithm of your choice, 
researching and deploying it on your own.

The choices (all in sklearn) are:
    
        1) K NEAREST NEIGHBOURS
        # classic, simple, easy to understand for machine learning beginners and useful
        
        2) ADABOOST (sometimes also called boosted decision tree)
        # Very powerful
        # Usually used with decision trees
        # ensemble method: meta classifier built from many decision trees
        # more advanced than k nearest neighbours
        
        3) RANDOM FOREST
        # Usually used with decision trees
        # ensemble method: meta classifier built from many decision trees
        # more advanced than k nearest neighbours
        
We canâ€™t check your results, because there are too many combinations of 
algorithms and parameters to check everything that you could try, but you have 
seen the accuracy that our previous algorithms (Naive Bayes, SVM, decision 
tree) achieved and can self-assess whether the new algorithm does better.

# FOLLOW THIS PROCESS when learning a new algorithm

    # 1) Research what the algorithm does on Google
    # 2) Find sklearn documentation and example code
    # 3) Get the algorithm running
    # 4) Make predictions 
    # 5) Evaluate the algorithm by calculating the accuracy                                                

"""

# For code see file your_algorithm.py in:
# C:\Users\User\Documents\S2DS_Bootcamp_2017\Online_course_notes\Udacity_Intro_to_Machine_Learning\ud120-projects\choose_your_own    

